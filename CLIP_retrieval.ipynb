{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets --quiet\n",
    "!pip install torchvision --quiet\n",
    "!pip install scikit-image --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install chromadb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_ID, device):\n",
    "\tmodel = CLIPModel.from_pretrained(model_ID).to(device)\n",
    "\tprocessor = CLIPProcessor.from_pretrained(model_ID)\n",
    "\ttokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
    "\treturn model, processor, tokenizer\n",
    "\n",
    "def get_single_image_embedding(my_image):\n",
    "  image = processor(\n",
    "      text = None,\n",
    "      images = my_image,\n",
    "      return_tensors=\"pt\"\n",
    "  )[\"pixel_values\"].to(device)\n",
    "  embedding = model.get_image_features(image)\n",
    "  return embedding\n",
    "\n",
    "def get_single_text_embedding(text):\n",
    "  inputs = tokenizer(text, return_tensors = \"pt\")\n",
    "  text_embeddings = model.get_text_features(**inputs)\n",
    "  return text_embeddings\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ID = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "model, processor, tokenizer = get_model_info(model_ID, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En utilisant le jeu de donn√©es sur HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"IGNF/FLAIR_1_osm_clip\"\n",
    "dataset = load_dataset(dataset_name, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuellement avec un mini sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
